{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running through a toy example to get a refresher on image segmentation\n",
    "- Loading a pretrained model\n",
    "- https://learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model with resnet101 backbone\n",
    "# NOTE: The .eval() method will load it in inference mode.\n",
    "fcn = models.segmentation.fcn_resnet101(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -nv https://static.independent.co.uk/s3fs-public/thumbnails/image/2018/04/10/19/pinyon-jay-bird.jpg -O bird.png\n",
    "img = Image.open('./bird.png')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the image\n",
    "# Apply the transformations needed \n",
    "import torchvision.transforms as T\n",
    "trf = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor(), T.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])]) \n",
    "# Compose is a great function that takes in a list in which each elemtn is of *transforms type\n",
    "# So we can pass compose a batch of images and all required transformations will be applied to the images.\n",
    "\n",
    "inp = trf(img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the network\n",
    "# Pass the input through the net \n",
    "out = fcn(inp)['out'] \n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "om = torch.argmax(out.squeeze(), dim=0).detach().cpu().numpy() \n",
    "print(om.shape)\n",
    "print(np.unique(om))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the helper function\n",
    "def decode_segmap(image, nc=21):\n",
    "  label_colors = np.array([(0, 0, 0),  # 0=background\n",
    "               # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\n",
    "               (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),\n",
    "               # 6=bus, 7=car, 8=cat, 9=chair, 10=cow\n",
    "               (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0),\n",
    "               # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person\n",
    "               (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128),\n",
    "               # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n",
    "               (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)])\n",
    "  r = np.zeros_like(image).astype(np.uint8)\n",
    "  g = np.zeros_like(image).astype(np.uint8)\n",
    "  b = np.zeros_like(image).astype(np.uint8)\n",
    "  for l in range(0, nc):\n",
    "    idx = image == l\n",
    "    r[idx] = label_colors[l, 0]\n",
    "    g[idx] = label_colors[l, 1]\n",
    "    b[idx] = label_colors[l, 2]\n",
    "  rgb = np.stack([r, g, b], axis=2)\n",
    "  return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = decode_segmap(om) \n",
    "plt.imshow(rgb); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(net, path, show_orig=True):\n",
    "  img = Image.open(path)\n",
    "  if show_orig:\n",
    "    plt.imshow(img); plt.axis('off'); plt.show()\n",
    "  # Comment the Resize and CenterCrop for better inference results\n",
    "  trf = T.Compose([T.Resize(256), \n",
    "                   T.CenterCrop(224), \n",
    "                   T.ToTensor(), \n",
    "                   T.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "                               std = [0.229, 0.224, 0.225])])\n",
    "  inp = trf(img).unsqueeze(0)\n",
    "  out = net(inp)['out']\n",
    "  om = torch.argmax(out.squeeze(), dim=0).detach().cpu().numpy()\n",
    "  rgb = decode_segmap(om)\n",
    "  plt.imshow(rgb); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment(fcn, './horse.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try segmentation using DeepLab\n",
    "# NOTE: Deeplab is a Semantic Segmentation Architecture that came out of Google Brain\n",
    "\n",
    "dlab = models.segmentation.deeplabv3_resnet101(pretrained=1).eval()\n",
    "\t\n",
    "segment(dlab, './horse.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('./dog-park.png')\n",
    "plt.imshow(img); plt.show()\n",
    "print ('Segmenatation Image on FCN')\n",
    "segment(fcn, path='./dog-park.png', show_orig=False)\n",
    "print ('Segmenatation Image on DeepLabv3')\n",
    "segment(dlab, path='./dog-park.png', show_orig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mars image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(net, path, show_orig=True):\n",
    "  img = Image.open(path)\n",
    "  if show_orig:\n",
    "    plt.imshow(img); plt.axis('off'); plt.show()\n",
    "  # Comment the Resize and CenterCrop for better inference results\n",
    "  trf = T.Compose([T.Resize(256), \n",
    "                   T.CenterCrop(224), \n",
    "                   T.ToTensor(), \n",
    "                   T.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "                               std = [0.229, 0.224, 0.225])\n",
    "                               ])\n",
    "  inp = trf(img).unsqueeze(0)\n",
    "  out = net(inp)['out']\n",
    "  om = torch.argmax(out.squeeze(), dim=0).detach().cpu().numpy()\n",
    "  rgb = decode_segmap(om)\n",
    "  plt.imshow(rgb); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../data/ai4mars-dataset-merged-0.1/msl/images/edr/NLA_397681398EDR_F0020000AUT_04096M1.JPG'\n",
    "img_path = './dog-park.png'\n",
    "img = Image.open(img_path)\n",
    "plt.imshow(img); plt.show()\n",
    "print ('Segmenatation Image on FCN')\n",
    "segment(fcn, path=img_path, show_orig=False)\n",
    "print ('Segmenatation Image on DeepLabv3')\n",
    "segment(dlab, path=img_path, show_orig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../data/ai4mars-dataset-merged-0.1/msl/images/edr/NLA_397681398EDR_F0020000AUT_04096M1.JPG'\n",
    "img = Image.open(img_path)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "\n",
    "i = skimage.io.imread('../data/ai4mars-dataset-merged-0.1/msl/images/edr/NLA_397681398EDR_F0020000AUT_04096M1.JPG')\n",
    "i.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image segmentation using DeeplabV3+ from PyTorch. \n",
    "- Starting first with a toy example on the face semgentation dataset \n",
    "- https://towardsdatascience.com/transfer-learning-for-segmentation-using-deeplabv3-in-pytorch-f770863d6a42\n",
    "- Then applying it to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "\n",
    "from src.segdataset import SegmentationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_dataset = SegmentationDataset(\"data/CrackForest\", \"Images\", \"Masks\", transforms=transforms.Compose([transforms.ToTensor()]))\n",
    "seg_dataloader = DataLoader(seg_dataset, batch_size=32, shuffle=False)\n",
    "samples = next(iter(seg_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(samples['image'].shape,\n",
    "samples['mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.ToPILImage()(samples['image'][5])\n",
    "mask = transforms.ToPILImage()(samples['mask'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting cross entropy to work\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "output = np.load('./output.npy', allow_pickle=True)\n",
    "masks = np.load('./masks.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.squeeze().shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running main.py in individual cells to break it down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import click\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from torch.utils import data\n",
    "\n",
    "import src.datahandler\n",
    "from src.model import createDeepLabv3\n",
    "from src.trainer import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = createDeepLabv3(outputchannels=10)\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torchvision import models\n",
    "\n",
    "outputchannels = 9\n",
    "\n",
    "model = models.segmentation.deeplabv3_resnet101(pretrained=True,\n",
    "                                                progress=True)\n",
    "model.classifier = DeepLabHead(2048, outputchannels)\n",
    "# Set the model in training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = Path(\"data/s5mars_data/resized\")\n",
    "# data_directory = Path(\"data/CrackForest\")\n",
    "exp_directory  = Path(\"experiments/testing_mars_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exp_directory.exists():\n",
    "    os.makedirs(exp_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the loss function\n",
    "# criterion = torch.nn.MSELoss(reduction='mean')\n",
    "criterion = torch.nn.CrossEntropyLoss() # is reduction='mean' the best here?\n",
    "# Specify the optimizer with a lower learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Specify the evaluation metrics\n",
    "metrics = {'f1_score': f1_score, 'auroc': roc_auc_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.datahandler as datahandler\n",
    "from src.trainer import train_model\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "# dataloaders = datahandler.get_dataloader_single_folder(\n",
    "#     data_directory, batch_size=batch_size)\n",
    "\n",
    "dataloaders = datahandler.get_dataloader_sep_folder(\n",
    "    data_directory, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE:  cuda:0\n",
      "Epoch 1/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape torch.Size([2, 9, 500, 500])\n",
      "masks shape torch.Size([2, 1, 500, 500])\n",
      "post squeeze torch.Size([2, 500, 500])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shoon\\repos\\DGMDE-17-Final-Project\\src\\trainer.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  masks = torch.tensor(masks, dtype=torch.long, device=device)\n",
      "  0%|          | 0/2500 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [500000, 4500000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m _ \u001b[39m=\u001b[39m train_model(model,\n\u001b[0;32m      2\u001b[0m                 criterion,\n\u001b[0;32m      3\u001b[0m                 dataloaders,\n\u001b[0;32m      4\u001b[0m                 optimizer,\n\u001b[0;32m      5\u001b[0m                 bpath\u001b[39m=\u001b[39;49mexp_directory,\n\u001b[0;32m      6\u001b[0m                 metrics\u001b[39m=\u001b[39;49mmetrics,\n\u001b[0;32m      7\u001b[0m                 num_epochs\u001b[39m=\u001b[39;49mepochs)\n",
      "File \u001b[1;32mc:\\Users\\Shoon\\repos\\DGMDE-17-Final-Project\\src\\trainer.py:86\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, dataloaders, optimizer, metrics, bpath, num_epochs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39mfor\u001b[39;00m name, metric \u001b[39min\u001b[39;00m metrics\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mf1_score\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     84\u001b[0m         \u001b[39m# Use a classification threshold of 0.1\u001b[39;00m\n\u001b[0;32m     85\u001b[0m         batchsummary[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mphase\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(\n\u001b[1;32m---> 86\u001b[0m             metric(y_true \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m, y_pred \u001b[39m>\u001b[39;49m \u001b[39m0.1\u001b[39;49m))\n\u001b[0;32m     87\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     88\u001b[0m         batchsummary[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mphase\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(\n\u001b[0;32m     89\u001b[0m             metric(y_true\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39muint8\u001b[39m\u001b[39m'\u001b[39m), y_pred))\n",
      "File \u001b[1;32mc:\\Users\\Shoon\\repos\\DGMDE-17-Final-Project\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1146\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf1_score\u001b[39m(\n\u001b[0;32m   1012\u001b[0m     y_true,\n\u001b[0;32m   1013\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1020\u001b[0m ):\n\u001b[0;32m   1021\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m \u001b[39m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[39m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m     \u001b[39mreturn\u001b[39;00m fbeta_score(\n\u001b[0;32m   1147\u001b[0m         y_true,\n\u001b[0;32m   1148\u001b[0m         y_pred,\n\u001b[0;32m   1149\u001b[0m         beta\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   1150\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1151\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[0;32m   1152\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[0;32m   1153\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1154\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[0;32m   1155\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Shoon\\repos\\DGMDE-17-Final-Project\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1287\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfbeta_score\u001b[39m(\n\u001b[0;32m   1159\u001b[0m     y_true,\n\u001b[0;32m   1160\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1167\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1168\u001b[0m ):\n\u001b[0;32m   1169\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \n\u001b[0;32m   1171\u001b[0m \u001b[39m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[39m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m     _, _, f, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[0;32m   1288\u001b[0m         y_true,\n\u001b[0;32m   1289\u001b[0m         y_pred,\n\u001b[0;32m   1290\u001b[0m         beta\u001b[39m=\u001b[39;49mbeta,\n\u001b[0;32m   1291\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1292\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[0;32m   1293\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[0;32m   1294\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mf-score\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[0;32m   1295\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1296\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[0;32m   1297\u001b[0m     )\n\u001b[0;32m   1298\u001b[0m     \u001b[39mreturn\u001b[39;00m f\n",
      "File \u001b[1;32mc:\\Users\\Shoon\\repos\\DGMDE-17-Final-Project\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1572\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1573\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[0;32m   1575\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Shoon\\repos\\DGMDE-17-Final-Project\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1374\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m average \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m average_options \u001b[39mand\u001b[39;00m average \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1372\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39maverage has to be one of \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1374\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m   1375\u001b[0m \u001b[39m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1376\u001b[0m \u001b[39m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m present_labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\Shoon\\repos\\DGMDE-17-Final-Project\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     60\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[39m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m     87\u001b[0m     type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m     type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Shoon\\repos\\DGMDE-17-Final-Project\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [500000, 4500000]"
     ]
    }
   ],
   "source": [
    "_ = train_model(model,\n",
    "                criterion,\n",
    "                dataloaders,\n",
    "                optimizer,\n",
    "                bpath=exp_directory,\n",
    "                metrics=metrics,\n",
    "                num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model, exp_directory / 'weights.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "081de2e0242d51b749964a799ab4f8179f089248fd37dbca336e34124b475fe7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
